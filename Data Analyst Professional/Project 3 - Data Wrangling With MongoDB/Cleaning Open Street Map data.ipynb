{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RIHAD VARIAWA, Data Scientist - Who has fun LEARNING, EXPLORING & GROWING\n",
    "import xml.etree.cElementTree as ET\n",
    "from pprint import pprint\n",
    "\n",
    "mumbai_file = \"../Data sets/mumbai.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1, 'node': 470060, 'relation': 126, 'way': 40396}\n"
     ]
    }
   ],
   "source": [
    "root_types = {}\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "    if element.tag == \"osm\":\n",
    "        for child in element:\n",
    "            if child.tag not in root_types:\n",
    "                root_types[child.tag] = 1\n",
    "            else:\n",
    "                root_types[child.tag] += 1\n",
    "pprint(root_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node defaultdict(<type 'set'>, {'tag': set(['k', 'v'])})\n",
      "relation defaultdict(<type 'set'>, {'member': set(['role', 'ref', 'type']), 'tag': set(['k', 'v'])})\n",
      "bounds defaultdict(<type 'set'>, {})\n",
      "way defaultdict(<type 'set'>, {'tag': set(['k', 'v']), 'nd': set(['ref'])})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sub_types = defaultdict(lambda : defaultdict(set))\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "    if element.tag in root_types:\n",
    "        for child in element:\n",
    "            for attribute in child.attrib:\n",
    "                sub_types[element.tag][child.tag].add(attribute)\n",
    "\n",
    "for types in root_types:\n",
    "    print types, sub_types[types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cable TV Provider\n",
      "EState Consultants\n",
      "Sahakari Bhandar\n",
      "business Park\n",
      "business Park\n",
      "Mangeshi Dham\n",
      "street parking\n",
      "street parking\n",
      "street parking\n",
      "street parking\n",
      "street parking\n",
      "street parking\n",
      "street parking\n",
      "Community Centre\n",
      "Area details \n",
      "street parking\n",
      "street parking\n",
      "Golden Park\n",
      "maneshi dham\n",
      "mangeshi dham\n",
      "{'lower': 108481, 'lower_colon': 3270, 'other': 842, 'problemchars': 20}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "keys = {'lower' : 0, 'lower_colon' : 0, 'problemchars' : 0, 'other' : 0}\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "    if element.tag == 'tag':\n",
    "        d = element.attrib['k']\n",
    "        flag = 0\n",
    "        if(re.match(lower,d)):\n",
    "            keys['lower']+=1\n",
    "            flag = 1\n",
    "        elif(re.match(lower_colon,d)):\n",
    "            keys['lower_colon']+=1\n",
    "            flag = 1\n",
    "        \n",
    "        if flag==0:\n",
    "            for c in d:\n",
    "                if(re.match(problemchars, c)):\n",
    "                    keys['problemchars']+=1\n",
    "                    flag = 2\n",
    "                    print d\n",
    "                    break\n",
    "                    \n",
    "        if flag ==0:\n",
    "            keys['other'] +=1\n",
    "        \n",
    "\n",
    "pprint(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['AND:importance_level',\n",
      "     'AND_a_c',\n",
      "     'AND_a_i',\n",
      "     'AND_a_nosr_p',\n",
      "     'AND_a_nosr_r',\n",
      "     'AND_a_w',\n",
      "     'AREA',\n",
      "     'Amenity',\n",
      "     'Area details ',\n",
      "     'Cable TV Provider',\n",
      "     'City',\n",
      "     'Community Centre',\n",
      "     'Consulate',\n",
      "     'EState Consultants',\n",
      "     'FIXME',\n",
      "     'Family',\n",
      "     'Golden Park',\n",
      "     'Gym',\n",
      "     'Mangeshi Dham',\n",
      "     'Name',\n",
      "     'Sahakari Bhandar',\n",
      "     'School',\n",
      "     'University',\n",
      "     'abutters',\n",
      "     'access',\n",
      "     'addr:city',\n",
      "     'addr:country',\n",
      "     'addr:full',\n",
      "     'addr:housename',\n",
      "     'addr:housenumber',\n",
      "     'addr:postcode',\n",
      "     'addr:state',\n",
      "     'addr:street',\n",
      "     'addr:unit',\n",
      "     'admin_level',\n",
      "     'aerodrome',\n",
      "     'aeroway',\n",
      "     'alt_name',\n",
      "     'alternative_name',\n",
      "     'amenity',\n",
      "     'ara',\n",
      "     'area',\n",
      "     'atm',\n",
      "     'attraction',\n",
      "     'barrier',\n",
      "     'bench',\n",
      "     'bicycle',\n",
      "     'boat',\n",
      "     'boundary',\n",
      "     'brand',\n",
      "     'bridge',\n",
      "     'bridge_name',\n",
      "     'bu',\n",
      "     'building',\n",
      "     'building:levels',\n",
      "     'bus',\n",
      "     'business Park',\n",
      "     'cables',\n",
      "     'capacity',\n",
      "     'charge',\n",
      "     'cmt',\n",
      "     'collection_times',\n",
      "     'construction',\n",
      "     'content',\n",
      "     'count',\n",
      "     'covered',\n",
      "     'created_by',\n",
      "     'crossing',\n",
      "     'cuisine',\n",
      "     'cutting',\n",
      "     'cycleway',\n",
      "     'date',\n",
      "     'denomination',\n",
      "     'description',\n",
      "     'design',\n",
      "     'designation',\n",
      "     'direction',\n",
      "     'dispensing',\n",
      "     'disused',\n",
      "     'drive_in',\n",
      "     'drive_through',\n",
      "     'earthquake_damage',\n",
      "     'ele',\n",
      "     'electrified',\n",
      "     'email',\n",
      "     'embankment',\n",
      "     'emergency',\n",
      "     'faculty',\n",
      "     'family',\n",
      "     'fax',\n",
      "     'fee',\n",
      "     'fixme',\n",
      "     'foot',\n",
      "     'footway',\n",
      "     'ford',\n",
      "     'frequency',\n",
      "     'fuel',\n",
      "     'fuel:HGV_diesel',\n",
      "     'fuel:biodiesel',\n",
      "     'fuel:biogas',\n",
      "     'fuel:cng',\n",
      "     'fuel:diesel',\n",
      "     'fuel:electricity',\n",
      "     'fuel:lpg',\n",
      "     'fuel:octane_91',\n",
      "     'fuel:octane_95',\n",
      "     'fuel:octane_98',\n",
      "     'gauge',\n",
      "     'girth',\n",
      "     'gns:dsg',\n",
      "     'gns:uni',\n",
      "     'goods',\n",
      "     'height',\n",
      "     'hgv',\n",
      "     'highway',\n",
      "     'historic',\n",
      "     'horse',\n",
      "     'iata',\n",
      "     'icao',\n",
      "     'incline',\n",
      "     'indoor',\n",
      "     'int_name',\n",
      "     'internet_access',\n",
      "     'internet_access:fee',\n",
      "     'is_capital',\n",
      "     'is_in',\n",
      "     'is_in:city',\n",
      "     'is_in:continent',\n",
      "     'is_in:country',\n",
      "     'is_in:country_code',\n",
      "     'is_in:county',\n",
      "     'is_in:state',\n",
      "     'junction',\n",
      "     'landcover',\n",
      "     'landmark',\n",
      "     'landuse',\n",
      "     'lanes',\n",
      "     'lay',\n",
      "     'layer',\n",
      "     'leisure',\n",
      "     'level',\n",
      "     'lit',\n",
      "     'loc_name',\n",
      "     'local_ref',\n",
      "     'man_made',\n",
      "     'managed',\n",
      "     'maneshi dham',\n",
      "     'mangeshi dham',\n",
      "     'maxheight',\n",
      "     'maxspeed',\n",
      "     'merge_id',\n",
      "     'microbrewery',\n",
      "     'military',\n",
      "     'motor_vehicle',\n",
      "     'motorcar',\n",
      "     'motorcycle',\n",
      "     'motorroad',\n",
      "     'name',\n",
      "     'name-hi',\n",
      "     'name:',\n",
      "     'name:bn',\n",
      "     'name:cs',\n",
      "     'name:de',\n",
      "     'name:en',\n",
      "     'name:es',\n",
      "     'name:fr',\n",
      "     'name:gu',\n",
      "     'name:hi',\n",
      "     'name:jbo',\n",
      "     'name:kn',\n",
      "     'name:ma',\n",
      "     'name:mr',\n",
      "     'name:pt',\n",
      "     'name:ru',\n",
      "     'name:sk',\n",
      "     'name:sr',\n",
      "     'name:ta',\n",
      "     'name:te',\n",
      "     'natural',\n",
      "     'new_name',\n",
      "     'no_name',\n",
      "     'noexit',\n",
      "     'noname',\n",
      "     'note',\n",
      "     'number',\n",
      "     'office',\n",
      "     'old_name',\n",
      "     'oneway',\n",
      "     'oneway:bicycle',\n",
      "     'opening_hours',\n",
      "     'operator',\n",
      "     'park_ride',\n",
      "     'parking',\n",
      "     'passengers',\n",
      "     'payment:bitcoin',\n",
      "     'phone',\n",
      "     'place',\n",
      "     'place:cca',\n",
      "     'place_name',\n",
      "     'platforms',\n",
      "     'population',\n",
      "     'postal_code',\n",
      "     'postcode',\n",
      "     'power',\n",
      "     'proposed',\n",
      "     'protect_class',\n",
      "     'public_access',\n",
      "     'public_transport',\n",
      "     'railway',\n",
      "     'rank',\n",
      "     'ref',\n",
      "     'ref:new',\n",
      "     'religion',\n",
      "     'rental',\n",
      "     'repair',\n",
      "     'residential',\n",
      "     'road',\n",
      "     'route',\n",
      "     'sale',\n",
      "     'seamark:fixme',\n",
      "     'seamark:landmark:height',\n",
      "     'seamark:light:1:colour',\n",
      "     'seamark:light:1:range',\n",
      "     'seamark:light:1:sector_end',\n",
      "     'seamark:light:1:sector_start',\n",
      "     'seamark:light:2:colour',\n",
      "     'seamark:light:2:range',\n",
      "     'seamark:light:2:sector_end',\n",
      "     'seamark:light:2:sector_start',\n",
      "     'seamark:light:3:colour',\n",
      "     'seamark:light:3:range',\n",
      "     'seamark:light:3:sector_end',\n",
      "     'seamark:light:3:sector_start',\n",
      "     'seamark:light:4:colour',\n",
      "     'seamark:light:4:range',\n",
      "     'seamark:light:4:sector_end',\n",
      "     'seamark:light:4:sector_start',\n",
      "     'seamark:light:category',\n",
      "     'seamark:light:character',\n",
      "     'seamark:light:colour',\n",
      "     'seamark:light:group',\n",
      "     'seamark:light:height',\n",
      "     'seamark:light:information',\n",
      "     'seamark:light:multiple',\n",
      "     'seamark:light:orientation',\n",
      "     'seamark:light:period',\n",
      "     'seamark:light:range',\n",
      "     'seamark:light:ref',\n",
      "     'seamark:light:sequence',\n",
      "     'seamark:longname',\n",
      "     'seamark:name',\n",
      "     'seamark:radar_transponder:category',\n",
      "     'seamark:radar_transponder:group',\n",
      "     'seamark:topmark',\n",
      "     'seamark:type',\n",
      "     'seasonal',\n",
      "     'service',\n",
      "     'shelter',\n",
      "     'ship:type',\n",
      "     'shirdi',\n",
      "     'shop',\n",
      "     'shop:type',\n",
      "     'sidewalk',\n",
      "     'smoking',\n",
      "     'social_facility',\n",
      "     'source',\n",
      "     'source:position',\n",
      "     'source:tracer',\n",
      "     'source:url',\n",
      "     'source:zoomlevel',\n",
      "     'sport',\n",
      "     'stars',\n",
      "     'start_date',\n",
      "     'step_count',\n",
      "     'steps',\n",
      "     'street parking',\n",
      "     'supervised',\n",
      "     'surface',\n",
      "     'sym',\n",
      "     'tactile_paving',\n",
      "     'taxi',\n",
      "     'tidal',\n",
      "     'toilets',\n",
      "     'toll',\n",
      "     'tourism',\n",
      "     'tower:type',\n",
      "     'tracktype',\n",
      "     'traffic_sign',\n",
      "     'trail_visibility',\n",
      "     'train',\n",
      "     'tunnel',\n",
      "     'turn:lanes',\n",
      "     'type',\n",
      "     'url',\n",
      "     'usage',\n",
      "     'vehicle',\n",
      "     'voltage',\n",
      "     'water',\n",
      "     'waterway',\n",
      "     'watland',\n",
      "     'website',\n",
      "     'wetland',\n",
      "     'wheelchair',\n",
      "     'width',\n",
      "     'wifi',\n",
      "     'wikipedia',\n",
      "     'wires',\n",
      "     'wp:foot',\n",
      "     'wp:highway',\n",
      "     'wp:rampatbeg',\n",
      "     'wp:rampatend',\n",
      "     'wpt_description',\n",
      "     'wpt_symbol'])\n"
     ]
    }
   ],
   "source": [
    "list_of_keys = set()\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "    if element.tag in [\"node\", \"way\"]:\n",
    "        for child in element:\n",
    "            try:\n",
    "                list_of_keys.add(child.attrib['k'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "pprint(list_of_keys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['Canteen',\n",
      "     'Educational Complex',\n",
      "     'Gym',\n",
      "     'Gymkhana',\n",
      "     'Society',\n",
      "     'Workshop',\n",
      "     'arts_centre',\n",
      "     'atm',\n",
      "     'bank',\n",
      "     'bar',\n",
      "     'bench',\n",
      "     'bicycle_rental',\n",
      "     'bureau_de_change',\n",
      "     'bus_station',\n",
      "     'cafe',\n",
      "     'car_wash',\n",
      "     'cinema',\n",
      "     'clinic',\n",
      "     'club',\n",
      "     'cold_storage',\n",
      "     'college',\n",
      "     'community_centre',\n",
      "     'courthouse',\n",
      "     'creamatorium',\n",
      "     'crematorium',\n",
      "     'cyber_cafe',\n",
      "     'doctors',\n",
      "     'drinking_water',\n",
      "     'fairgrounds',\n",
      "     'fast_food',\n",
      "     'ferry_terminal',\n",
      "     'fire_station',\n",
      "     'food_court',\n",
      "     'fountain',\n",
      "     'fuel',\n",
      "     'fuel; kiosk',\n",
      "     'grave_yard',\n",
      "     'gym',\n",
      "     'hospital',\n",
      "     'ice_cream',\n",
      "     'kindergarten',\n",
      "     'library',\n",
      "     'market',\n",
      "     'marketplace',\n",
      "     'nightclub',\n",
      "     'parking',\n",
      "     'parking;fuel',\n",
      "     'pharmacy',\n",
      "     'picnic spot',\n",
      "     'pl',\n",
      "     'place_of_worship',\n",
      "     'police',\n",
      "     'post_box',\n",
      "     'post_office',\n",
      "     'prison',\n",
      "     'pub',\n",
      "     'public_building',\n",
      "     'restaurant',\n",
      "     'restroom',\n",
      "     'school',\n",
      "     'shelter',\n",
      "     'shop',\n",
      "     'social_centre',\n",
      "     'social_facility',\n",
      "     'studio',\n",
      "     'swimming_pool',\n",
      "     'taxi',\n",
      "     'tea_shop',\n",
      "     'telephone',\n",
      "     'theatre',\n",
      "     'toilets',\n",
      "     'townhall',\n",
      "     'university',\n",
      "     'vending_machine',\n",
      "     'waste_transfer_station',\n",
      "     'water'])\n"
     ]
    }
   ],
   "source": [
    "list_of_amenities = set()\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "    if element.tag in [\"node\", \"way\"]:\n",
    "        for child in element:\n",
    "            try:\n",
    "                if \"amenity\" in child.attrib['k']:\n",
    "                    list_of_amenities.add(child.attrib['v'])\n",
    "                if \"Amenity\" in child.attrib['k']:\n",
    "                    list_of_amenities.add(child.attrib['v'])\n",
    "            except:\n",
    "                pass\n",
    "           \n",
    "\n",
    "pprint(list_of_amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AND': 'AND',\n",
      " 'addr': 'addr',\n",
      " 'building': 'building',\n",
      " 'fuel': 'fuel',\n",
      " 'gns': 'gns',\n",
      " 'internet_access': 'internet_access',\n",
      " 'is_in': 'is_in',\n",
      " 'name': 'name',\n",
      " 'oneway': 'oneway',\n",
      " 'payment': 'payment',\n",
      " 'place': 'place',\n",
      " 'ref': 'ref',\n",
      " 'seamark': 'seamark',\n",
      " 'ship': 'ship',\n",
      " 'shop': 'shop',\n",
      " 'source': 'source',\n",
      " 'tower': 'tower',\n",
      " 'turn': 'turn',\n",
      " 'wp': 'wp'}\n"
     ]
    }
   ],
   "source": [
    "keys_with_sub_types = {}\n",
    "keys_with_sub_types_set = set()\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "    if element.tag in root_types:\n",
    "        for child in element:\n",
    "            try:\n",
    "                if ':' in child.attrib['k']:\n",
    "                    parent = child.attrib['k'].split(':')[0]\n",
    "                    if parent not in keys_with_sub_types:\n",
    "                        keys_with_sub_types[parent] = parent\n",
    "                        keys_with_sub_types_set.add(parent)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "pprint(keys_with_sub_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AND': 'AND',\n",
      " 'addr': 'address',\n",
      " 'building': 'building',\n",
      " 'fuel': 'fuel',\n",
      " 'gns': 'GEOnet Name Server',\n",
      " 'internet_access': 'internet_access',\n",
      " 'is_in': 'located_in',\n",
      " 'name': 'name',\n",
      " 'oneway': 'oneway',\n",
      " 'payment': 'payment',\n",
      " 'place': 'place',\n",
      " 'ref': 'reference',\n",
      " 'seamark': 'seamark',\n",
      " 'ship': 'ship',\n",
      " 'shop': 'shop',\n",
      " 'source': 'source',\n",
      " 'tower': 'tower',\n",
      " 'turn': 'turn',\n",
      " 'wp': 'wp'}\n"
     ]
    }
   ],
   "source": [
    "keys_with_sub_types['addr'] = \"address\"\n",
    "keys_with_sub_types['gns']  = \"GEOnet Name Server\"\n",
    "keys_with_sub_types['is_in'] = \"located_in\"\n",
    "keys_with_sub_types['ref'] = \"reference\"\n",
    "pprint(keys_with_sub_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['',\n",
      " 'bn',\n",
      " 'cs',\n",
      " 'de',\n",
      " 'en',\n",
      " 'es',\n",
      " 'fr',\n",
      " 'gu',\n",
      " 'hi',\n",
      " 'jbo',\n",
      " 'kn',\n",
      " 'ma',\n",
      " 'mr',\n",
      " 'pl',\n",
      " 'pt',\n",
      " 'ru',\n",
      " 'sk',\n",
      " 'sr',\n",
      " 'ta',\n",
      " 'te']\n"
     ]
    }
   ],
   "source": [
    "languages_mapping = {}\n",
    "languages = []\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "    if element.tag in root_types:\n",
    "        for child in element:\n",
    "            try:\n",
    "                if ':' in child.attrib['k']:\n",
    "                    parent_field = child.attrib['k'].split(':')[0]\n",
    "                    sub_field = child.attrib['k'].split(':')[1]\n",
    "                    if parent_field == \"name\":\n",
    "                        if sub_field not in languages:\n",
    "                            languages_mapping[sub_field] = sub_field\n",
    "                            languages.append(sub_field)\n",
    "        \n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "pprint(sorted(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 'other',\n",
      " 'bn': 'bengali',\n",
      " 'cs': 'czech',\n",
      " 'de': 'german',\n",
      " 'en': 'english',\n",
      " 'es': 'spanish',\n",
      " 'fr': 'french',\n",
      " 'gu': 'gujrati',\n",
      " 'hi': 'hindi',\n",
      " 'jbo': 'lobjan',\n",
      " 'kn': 'kanada',\n",
      " 'ma': 'arabic',\n",
      " 'mr': 'marathi',\n",
      " 'pl': 'polish',\n",
      " 'pt': 'portuguese',\n",
      " 'ru': 'russian',\n",
      " 'sk': 'slovak',\n",
      " 'sr': 'serbian',\n",
      " 'ta': 'tamil',\n",
      " 'te': 'telugu'}\n"
     ]
    }
   ],
   "source": [
    "languages_replace = [\"other\",\"bengali\",\"czech\",\"german\",\"english\",\"spanish\",\"french\",\"gujrati\",\"hindi\",\"lobjan\",\"kanada\",\"arabic\",\"marathi\",\"polish\",\"portuguese\",\"russian\",\"slovak\",\"serbian\",\"tamil\", \"telugu\"]\n",
    "counter = 0\n",
    "\n",
    "for language in sorted(languages):\n",
    "    languages_mapping[language] = languages_replace[counter]\n",
    "    counter += 1\n",
    "\n",
    "pprint(languages_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['400001',\n",
      "     '400002',\n",
      "     '400003',\n",
      "     '400004',\n",
      "     '400005',\n",
      "     '400006',\n",
      "     '400007',\n",
      "     '400008',\n",
      "     '400009',\n",
      "     '400011',\n",
      "     '400013',\n",
      "     '400018',\n",
      "     '400020',\n",
      "     '400021',\n",
      "     '400023',\n",
      "     '400026',\n",
      "     '40003',\n",
      "     '400034',\n",
      "     '400036',\n",
      "     '40013',\n",
      "     '400702',\n",
      "     '401200',\n",
      "     '401204',\n",
      "     '402106',\n",
      "     '402200',\n",
      "     '402202',\n",
      "     '402203',\n",
      "     '402204',\n",
      "     '410200',\n",
      "     '410201',\n",
      "     '410202',\n",
      "     '410205',\n",
      "     '410210',\n",
      "     '410218',\n",
      "     '410300',\n",
      "     '410400',\n",
      "     '410401',\n",
      "     '421100',\n",
      "     '421301',\n",
      "     '421302',\n",
      "     '421501',\n",
      "     '421503',\n",
      "     '421601',\n",
      "     '421604'])\n",
      "set([' 410201',\n",
      "     '400 022',\n",
      "     '400 601',\n",
      "     '400001',\n",
      "     '400005',\n",
      "     '400007',\n",
      "     '400010',\n",
      "     '400013',\n",
      "     '400016',\n",
      "     '400018',\n",
      "     '400019',\n",
      "     '400020',\n",
      "     '400021',\n",
      "     '400022',\n",
      "     '400030',\n",
      "     '400033',\n",
      "     '400034',\n",
      "     '400035',\n",
      "     '400036',\n",
      "     '400038',\n",
      "     '400039',\n",
      "     '400043',\n",
      "     '400047',\n",
      "     '400049',\n",
      "     '400050',\n",
      "     '400052',\n",
      "     '400053',\n",
      "     '400054',\n",
      "     '400056',\n",
      "     '400057',\n",
      "     '400058',\n",
      "     '4000607',\n",
      "     '400061',\n",
      "     '400062',\n",
      "     '400063',\n",
      "     '400064',\n",
      "     '400066',\n",
      "     '400067',\n",
      "     '400068',\n",
      "     '400069',\n",
      "     '400071',\n",
      "     '400072',\n",
      "     '400074',\n",
      "     '400076',\n",
      "     '400076, India',\n",
      "     '400077',\n",
      "     '400078',\n",
      "     '400080',\n",
      "     '400086',\n",
      "     '400087',\n",
      "     '400088',\n",
      "     '400089',\n",
      "     '400091',\n",
      "     '400093',\n",
      "     '400096',\n",
      "     '400098',\n",
      "     '400101',\n",
      "     '400102',\n",
      "     '400103',\n",
      "     '40049',\n",
      "     '40058',\n",
      "     '400601',\n",
      "     '400606',\n",
      "     '400607',\n",
      "     '400610',\n",
      "     '400614',\n",
      "     '400705',\n",
      "     '400706',\n",
      "     '401104 ',\n",
      "     '401202',\n",
      "     '401203',\n",
      "     '401303',\n",
      "     '410 201',\n",
      "     '410210',\n",
      "     '410701',\n",
      "     '421202',\n",
      "     '421501',\n",
      "     '421503',\n",
      "     '63103'])\n"
     ]
    }
   ],
   "source": [
    "postal_code = set()\n",
    "postcode = set()\n",
    "\n",
    "for event, element in ET.iterparse(mumbai_file):\n",
    "        for child in element:\n",
    "            try:\n",
    "                if child.attrib['k'] == \"postal_code\":\n",
    "                    postal_code.add(child.attrib['v'])\n",
    "                if child.attrib['k'] == \"addr:postcode\":\n",
    "                    postcode.add(child.attrib['v'])\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "pprint(postal_code)\n",
    "pprint(postcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def shape_data(elements , current_dict):\n",
    "    \n",
    "                                       #Head tags, decide the type of entry\n",
    "    if elements.tag in [\"node\",\"way\"]:                             \n",
    "        current_dict['type'] = elements.tag\n",
    "        \n",
    "        \n",
    "                                      \n",
    "                                      # created field extracted from attributes of head tag\n",
    "        current_dict['created'] = {}                               \n",
    "        if 'lat' in elements.attrib and 'lon' in elements.attrib: \n",
    "            current_dict['pos']  =  [float(elements.attrib['lat']), float(elements.attrib['lon'])]\n",
    "            \n",
    "                                       \n",
    "                                       # Other attributes contained in the head tag are added to the dictionary\n",
    "        for attribute in elements.attrib:\n",
    "            if attribute not in [\"lat\", \"lon\", \"user\", \"uid\", \"timestamp\", \"changeset\", \"version\"]:\n",
    "                current_dict[attribute] = elements.attrib[attribute]\n",
    "            elif attribute in [\"user\", \"uid\", \"timestamp\", \"changeset\", \"version\"]:\n",
    "                current_dict['created'][attribute] = elements.attrib[attribute]\n",
    "    \n",
    "                              \n",
    "    #'nd' tag of the way type of head tags are found and its ref attribute is added to the dictionary in a list\n",
    "    if elements.tag == \"nd\":\n",
    "        if 'nd_ref' not in current_dict:\n",
    "            current_dict['nd_ref'] = []\n",
    "        if 'ref' in elements.attrib:    \n",
    "            current_dict['nd_ref'].append(elements.attrib['ref'])\n",
    "    \n",
    "    \n",
    "    # Child tags containing head tag as 'tag' are scanned one by one\n",
    "    for element in elements:\n",
    "                                                    \n",
    "        if element.tag == \"tag\":\n",
    "                                                    \n",
    "            # Amentities are converted to a list as some contain multiple \n",
    "            if element.attrib['k'] in [\"Amenity\",\"amenity\"]:\n",
    "                if ';' in element.attrib['v']:\n",
    "                    values = element.attrib['v'].split(';')\n",
    "                    current_dict['amenity'] = values\n",
    "                else:\n",
    "                    current_dict['amenity'] = [element.attrib['v']]  \n",
    "                                \n",
    "            \n",
    "            #  Postal code is a duplicate field, it is inserted into the address field                                         \n",
    "            elif element.attrib['k'] == \"postal_code\":\n",
    "                    if 'address' not in current_dict:\n",
    "                        current_dict[address] = {}\n",
    "                        current_dict['address']['postcode'] = int(element.attrib['v'])\n",
    "\n",
    "            # another duplicate field for addr:postcode                                      \n",
    "            elif element.attrib['k'] == \"postcode\":\n",
    "                if 'address' not in current_dict:\n",
    "                        current_dict[address] = {}\n",
    "                current_dict['address']['postcode'] = int(element.attrib['v'])\n",
    "\n",
    "            # Fields containing sub fields are scanned                                      \n",
    "            elif ':' in element.attrib['k']:\n",
    "                \n",
    "                 # split the name by ':' to get the parent field and its sub field\n",
    "                splitted_fields = element.attrib['k'].split(':')  \n",
    "                parent = splitted_fields[0]\n",
    "\n",
    "                # If parent field isn't present inintialise\n",
    "                # it as an empty dictionary\n",
    "                # Abbreviated names are changed to their full\n",
    "                # names from keys_with_sub_types variable\n",
    "                if keys_with_sub_types[parent] not in current_dict:  \n",
    "                    current_dict[keys_with_sub_types[parent]] = {}\n",
    "                elif type(current_dict[keys_with_sub_types[parent]]) != type({}):      \n",
    "                    value = current_dict[keys_with_sub_types[parent]]                  \n",
    "                    current_dict[keys_with_sub_types[parent]] = {}                     \n",
    "                    current_dict[keys_with_sub_types[parent]][parent] = value          \n",
    "                \n",
    "                if parent == \"addr\" and element.attrib('k').split(':')[1] == \"postcode\":\n",
    "                    \n",
    "                    current_dict[keys_with_sub_types[parent]]['postcode'] = element.attrib('v').strip(' ').split(',')[0]\n",
    "                    \n",
    "                \n",
    "                # If the parent field is name, get its\n",
    "                # sub field and insert with suitable full name of \n",
    "                # the language from language_mapping variable\n",
    "                elif parent == \"name\":\n",
    "                    sub_name = splitted_fields[1]\n",
    "                    current_dict[keys_with_sub_types[parent]][languages_mapping[sub_name]] = element.attrib['v']\n",
    "\n",
    "                elif len(splitted_fields) > 2:\n",
    "                    temp_dict  = {}\n",
    "                    temp_dict1 = {}\n",
    "                    temp_dict1[splitted_fields[-1]] = element.attrib['v']             \n",
    "                                                                                       \n",
    "                    for i in range(2,len(splitted_fields)):                            \n",
    "                        temp_dict[splitted_fields[-1*i]] = temp_dict1\n",
    "                        temp_dict1 = temp_dict\n",
    "\n",
    "                    current_dict[keys_with_sub_types[parent]] = temp_dict\n",
    "\n",
    "                else:\n",
    "                    current_dict[keys_with_sub_types[parent]][splitted_fields[1]] = element.attrib['v']\n",
    "            \n",
    "            # for any other parent field insert with suitable\n",
    "            # corrections from keys_with_sub_types\n",
    "            else:\n",
    "                attribute = element.attrib['k']\n",
    "                attribute = attribute.lower()\n",
    "                try:\n",
    "                    if type(current_dict[attribute]) == type({}):                       \n",
    "                        current_dict[attribute][attribute] = element.attrib['v']        \n",
    "                    else:\n",
    "                        current_dict[attribute] = element.attrib['v']\n",
    "                except:\n",
    "                    current_dict[attribute] = element.attrib['v']\n",
    "\n",
    "                \n",
    "    return current_dict\n",
    "\n",
    "\n",
    "\n",
    "def create_dictionary(mumbai_file):\n",
    "    \n",
    "    json_list = []\n",
    "    current_dict = {}\n",
    "    \n",
    "    for event, element in ET.iterparse(mumbai_file):\n",
    "        \n",
    "        # as soon as new node/way is detected append the current dictionary\n",
    "        # and clear it fit the contents of the new node/way.\n",
    "        if element.tag in [\"node\", \"way\"]:\n",
    "            json_list.append(current_dict)                  \n",
    "            current_dict = {}                               \n",
    "            current_dict = shape_data(element, current_dict)\n",
    "        elif element.tag not in [\"relation\", \"bounds\"]:\n",
    "            current_dict = shape_data(element, current_dict)\n",
    "            \n",
    "    return json_list\n",
    "\n",
    "final_dictionary = create_dictionary(mumbai_file)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json', 'w') as file:\n",
    "    json.dump(final_dictionary, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Json File:  105 MB\n",
      "Size of uncompressed osm File :  92 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print \"Size of Json File: \" ,(os.path.getsize('data.json'))/(1024*1024) , \"MB\"\n",
    "print \"Size of uncompressed osm File : \", (os.path.getsize(mumbai_file))/(1024*1024) , \"MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "connection = pymongo.MongoClient(\"mongodb://localhost\")\n",
    "\n",
    "db = connection.osm_data\n",
    "\n",
    "record = db.mumbai_data\n",
    "\n",
    "mumbai_data = open('data.json', 'r')\n",
    "\n",
    "parsed_mumbai_data = json.loads(mumbai_data.read())\n",
    "\n",
    "for entry in parsed_mumbai_data:\n",
    "    record.insert_one(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510456"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510456"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_mumbai_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'node', u'count': 470053}, {u'_id': u'way', u'count': 40213}]\n"
     ]
    }
   ],
   "source": [
    "entry_types_count = record.aggregate([{\"$match\": {\"type\": {\"$in\": [\"node\",\"way\"]}}},{ \"$group\" : {\"_id\": \"$type\", \"count\" : {\"$sum\": 1} }}])\n",
    "\n",
    "pprint(list(entry_types_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455\n"
     ]
    }
   ],
   "source": [
    "pprint(len(list(record.distinct('created.user'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'PlaneMad', u'count': 65571},\n",
      " {u'_id': u'MJL Wood', u'count': 61257},\n",
      " {u'_id': u'balaji88', u'count': 59941},\n",
      " {u'_id': u'parambyte', u'count': 45287},\n",
      " {u'_id': u'udaya', u'count': 44663},\n",
      " {u'_id': u'smith_dsm', u'count': 39375},\n",
      " {u'_id': u'Giyavudeen', u'count': 30007},\n",
      " {u'_id': u'indigomc', u'count': 19682},\n",
      " {u'_id': u'shekhar', u'count': 13296},\n",
      " {u'_id': u'Moorthy1', u'count': 11864},\n",
      " {u'_id': u'singleton', u'count': 10324},\n",
      " {u'_id': u'PremK', u'count': 10180},\n",
      " {u'_id': u'dmgroom_coastlines', u'count': 7880},\n",
      " {u'_id': u'gaurav jain', u'count': 7754},\n",
      " {u'_id': u'Heinz_V', u'count': 6907},\n",
      " {u'_id': u'Oberaffe', u'count': 6393},\n",
      " {u'_id': u'Meghanand', u'count': 6135},\n",
      " {u'_id': u'Shekhar11', u'count': 4717},\n",
      " {u'_id': u'jain zachariah', u'count': 4545},\n",
      " {u'_id': u'katpatuka', u'count': 4402}]\n"
     ]
    }
   ],
   "source": [
    "user_contributions = record.aggregate([{\"$match\": {\"created.user\": {\"$exists\": 1}}},\n",
    "                                       {\"$group\": {\"_id\" : \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "                                      {\"$sort\": {\"count\" : -1}},\n",
    "                                      {\"$limit\": 20}])\n",
    "pprint(list(user_contributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'italian', u'count': 5},\n",
      " {u'_id': u'indian', u'count': 30},\n",
      " {u'_id': u'coffee_shop', u'count': 26},\n",
      " {u'_id': u'mediterranean', u'count': 1},\n",
      " {u'_id': u'burger', u'count': 12},\n",
      " {u'_id': u'pizza', u'count': 10},\n",
      " {u'_id': u'persian', u'count': 1},\n",
      " {u'_id': u'vegetarian', u'count': 14},\n",
      " {u'_id': u'seafood', u'count': 2},\n",
      " {u'_id': u'sandwich', u'count': 4},\n",
      " {u'_id': u'regional', u'count': 3},\n",
      " {u'_id': u'chinese', u'count': 7},\n",
      " {u'_id': u'chicken', u'count': 2},\n",
      " {u'_id': u'american', u'count': 2},\n",
      " {u'_id': u'ice_cream', u'count': 2},\n",
      " {u'_id': u'asian', u'count': 1},\n",
      " {u'_id': u'international', u'count': 5},\n",
      " {u'_id': u'thai', u'count': 1},\n",
      " {u'_id': u'spanish', u'count': 1},\n",
      " {u'_id': u'sad_food', u'count': 1},\n",
      " {u'_id': u'Goan', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(record.aggregate([{\"$match\": {'cuisine': {\"$exists\": 1}}},\n",
    "                         {\"$group\": {'_id': \"$cuisine\", \"count\" : {\"$sum\": 1}}}])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': [u'place_of_worship'], u'count': 246},\n",
      " {u'_id': [u'school'], u'count': 211},\n",
      " {u'_id': [u'restaurant'], u'count': 161},\n",
      " {u'_id': [u'bank'], u'count': 134},\n",
      " {u'_id': [u'parking'], u'count': 117},\n",
      " {u'_id': [u'hospital'], u'count': 110},\n",
      " {u'_id': [u'bus_station'], u'count': 102},\n",
      " {u'_id': [u'fuel'], u'count': 100},\n",
      " {u'_id': [u'college'], u'count': 83},\n",
      " {u'_id': [u'fast_food'], u'count': 66},\n",
      " {u'_id': [u'police'], u'count': 57},\n",
      " {u'_id': [u'cafe'], u'count': 55},\n",
      " {u'_id': [u'atm'], u'count': 51},\n",
      " {u'_id': [u'cinema'], u'count': 50},\n",
      " {u'_id': [u'swimming_pool'], u'count': 48},\n",
      " {u'_id': [u'post_office'], u'count': 40},\n",
      " {u'_id': [u'pharmacy'], u'count': 36},\n",
      " {u'_id': [u'toilets'], u'count': 35},\n",
      " {u'_id': [u'marketplace'], u'count': 27},\n",
      " {u'_id': [u'public_building'], u'count': 22}]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(record.aggregate([{\"$match\": {'amenity': {\"$exists\": 1}}},\n",
    "                         {\"$group\": {'_id': \"$amenity\", \"count\" : {\"$sum\": 1}}},\n",
    "                             {\"$sort\":{\"count\": -1}},\n",
    "                             {\"$limit\": 20}])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'yes', u'count': 6222},\n",
      " {u'_id': u'residential', u'count': 485},\n",
      " {u'_id': u'apartments', u'count': 305},\n",
      " {u'_id': u'office', u'count': 76},\n",
      " {u'_id': u'train_station', u'count': 58},\n",
      " {u'_id': u'house', u'count': 57},\n",
      " {u'_id': u'industrial', u'count': 53},\n",
      " {u'_id': u'concourse', u'count': 40},\n",
      " {u'_id': u'commercial', u'count': 17},\n",
      " {u'_id': u'roof', u'count': 9}]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(record.aggregate([{\"$match\": {'building': {\"$exists\": 1}}},\n",
    "                         {\"$group\": {'_id': \"$building\", \"count\" : {\"$sum\": 1}}},\n",
    "                             {\"$sort\":{\"count\": -1}},\n",
    "                             {\"$limit\": 10}])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'400057', u'count': 135},\n",
      " {u'_id': u'400053', u'count': 63},\n",
      " {u'_id': u'400050', u'count': 56},\n",
      " {u'_id': u'400061', u'count': 30},\n",
      " {u'_id': u'400607', u'count': 29},\n",
      " {u'_id': u'400049', u'count': 12},\n",
      " {u'_id': u'400071', u'count': 8},\n",
      " {u'_id': u'400088', u'count': 8},\n",
      " {u'_id': u'400601', u'count': 7},\n",
      " {u'_id': u'400058', u'count': 7}]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(record.aggregate([{\"$match\": {'address.postcode': {\"$exists\": 1}}},\n",
    "                         {\"$group\": {'_id': \"$address.postcode\", \"count\" : {\"$sum\": 1}}},\n",
    "                             {\"$sort\":{\"count\": -1}},\n",
    "                             {\"$limit\": 10}])))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
