{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 4: Dynamic Programming\n",
    "\n",
    "### Exercise 4.2 Policy Evaluation using in-place method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Evaluation calculates the value function for a policy, given the policy and the full definition of the associated Markov Decision Process.  The full definition of an MDP is the set of states, the set of available actions for each state, the set of rewards, the discount factor, and the state/reward transition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import test_dp               # required for testing and grading your code\n",
    "import gridworld_mdp as gw   # defines the MDP for a 4x4 gridworld\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the algorithm for Iterative Policy Evaluation using the in-place approach**. In the in-place approach, one array holds the values being estimated for each state and the same array is used for estimates of states needed by the algorithm.\n",
    "\n",
    "A empty function **policy_eval_in_place** is provided below; implement the body of the function to correctly calculate the value of the policy using the inplace approach. The function defines 5 parameters - a definition of each parameter is given in the comment block for the function. For sample parameter values, see the calling code in the cell following the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_in_place(state_count, gamma, theta, get_policy, get_transitions):\n",
    "    \"\"\"\n",
    "    This function uses the inplace approach to evaluate the specified policy for the specified MDP:\n",
    "    \n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    \n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    \n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    \n",
    "    'get_policy' is the stochastic policy function - it takes a state parameter and returns list of tuples, \n",
    "        where each tuple is of the form: (action, probability).  It represents the policy being evaluated.\n",
    "        \n",
    "    'get_transitions' is the state/reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "         \n",
    "    \"\"\"\n",
    "    V = np.array(state_count*[0], copy = True, dtype = np.float64)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(state_count):\n",
    "            v = 0            \n",
    "            for action, action_prob in get_policy(state):                \n",
    "                for next_state, reward, probability in get_transitions(state, action):\n",
    "                    v += action_prob * probability * (reward + gamma * V[next_state])\n",
    "                \n",
    "            delta = max(delta, np.abs(V[state] - v))\n",
    "            V[state] = v\n",
    "          \n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return [e for e in V]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, test our function using the MDP defined by gw.* functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values= [0.0, -5.2759064856003022, -7.1258036673723248, -7.6477299227176614, -5.2759064856003022, -6.6042139132509767, -7.1785079112764745, -7.1263842436560916, -7.1258036673723248, -7.1785079112764754, -6.6046783717757869, -5.2766639943228588, -7.6477299227176623, -7.1263842436560925, -5.2766639943228597]\n"
     ]
    }
   ],
   "source": [
    "def get_equal_policy(state):\n",
    "    # build a simple policy where all 4 actions have the same probability, ignoring the specified state\n",
    "    policy = ( (\"up\", .25), (\"right\", .25), (\"down\", .25), (\"left\", .25))\n",
    "    return policy\n",
    "\n",
    "n_states = gw.get_state_count()\n",
    "\n",
    "# test our function\n",
    "values = policy_eval_in_place(state_count=n_states, gamma=.9, theta=.001, get_policy=get_equal_policy, \\\n",
    "    get_transitions=gw.get_transitions)\n",
    "\n",
    "print(\"Values=\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output from running above cell:**\n",
    "\n",
    "`\n",
    "Values= [0.0, -5.275906485600302, -7.125803667372325, -7.647729922717661, -5.275906485600302, -6.604213913250977, -7.1785079112764745, -7.126384243656092, -7.125803667372325, -7.178507911276475, -6.604678371775787, -5.276663994322859, -7.647729922717662, -7.1263842436560925, -5.27666399432286]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test our function using the test_dp helper.  The helper also uses the gw MDP, but with a different gamma value.\n",
    "If our function passes all tests, a passcode will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Evaluation (in-place)\n",
      "passed test: return value is list\n",
      "passed test: length of list = 15\n",
      "passed test: values of list elements\n",
      "PASSED: Policy Evaluation (in-place) passcode = 9991-562\n"
     ]
    }
   ],
   "source": [
    "# test our function using the test_db helper\n",
    "test_dp.policy_eval_in_place_test(policy_eval_in_place)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
