{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 4: Dynamic Programming\n",
    "\n",
    "### Exercise 4.3 Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Iteration calculates the optimal policy for an MDP, given its full definition.  The full definition of an MDP is the set of states, the set of available actions for each state, the set of rewards, the discount factor, and the state/reward transition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import test_dp               # required for testing and grading your code\n",
    "import gridworld_mdp as gw   # defines the MDP for a 4x4 gridworld\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the algorithm for Policy Iteration**.  Policy Iteration calculates the optimal policy for an MDP by doing repeated steps of policy evaluation and policy improvement.\n",
    "\n",
    "A empty function **policy_iteration** is provided below; implement the body of the function to correctly calculate the optimal policy for an MDP.  The function defines 5 parameters - a definition of each parameter is given in the comment block for the function.  For sample parameter values, see the calling code in the cell following the function.\n",
    "\n",
    "Note that there is a subtle difference between the algorithm for Policy Evaluation, which assumes the policy is stochastic, and the Policy Evaluation step for the Policy Iteration algorithm, which assumes the policy is deterministic.  This means that you cannot directly call your previous code, but you can reuse large pieces of it for the Policy Evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n",
    "    \"\"\"\n",
    "    This function computes the optimal value function and policy for the specified MDP, using the Policy Iteration algorithm.\n",
    "    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n",
    "    \n",
    "    'gamma' is the MDP discount factor for rewards.\n",
    "    \n",
    "    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n",
    "    \n",
    "    'get_available_actions' returns a list of the MDP available actions for the specified state parameter.\n",
    "    \n",
    "    'get_transitions' is the MDP state / reward transiton function.  It accepts two parameters, state and action, and returns\n",
    "        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n",
    "    \"\"\"\n",
    "    V = state_count*[0]              # init all state value estimates to 0\n",
    "    pi = state_count * [0]\n",
    "    \n",
    "    # init with a policy with first avail action for each state\n",
    "    for s in range(state_count):\n",
    "        avail_actions = get_available_actions(s)\n",
    "        pi[s] = avail_actions[0]\n",
    "     \n",
    "    #Evaluates the value of a state given the current action from the state\n",
    "    def evaluate_policy(values, actions):\n",
    "        while True:        \n",
    "            delta = 0\n",
    "            for state in range(state_count):                \n",
    "                next_state, reward, probability = get_transitions(state, actions[state])[0]\n",
    "                v = probability * (reward + gamma * values[next_state])\n",
    "                delta = max(delta, np.abs(values[state] - v))\n",
    "                V[state] = v        \n",
    "            \n",
    "            if delta < theta:\n",
    "                break\n",
    "         \n",
    "        return V\n",
    "\n",
    "    while True:        \n",
    "\n",
    "        #Policy Evaluation\n",
    "        V = evaluate_policy(V, pi)\n",
    "        \n",
    "        \n",
    "        #Policy improvement   \n",
    "        #Once we have a policy, we choose a policy from the available actions that is the best move from the \n",
    "        #current state, if thats different from the existing actions, we will need to evaluate the policy\n",
    "        #again with the new set of actions from the state. We stop when two subsequent improvements gives us the \n",
    "        #same action\n",
    "        \n",
    "        policy_stable = True    \n",
    "        for state in range(state_count):            \n",
    "            available_actions = [act for act in get_available_actions(state)] \n",
    "            #For each state and action, get possible next state's values. We use these next states to find max value\n",
    "            neighbor_values = [V[get_transitions(state, a)[0][0]] for a in available_actions]\n",
    "            greedy_transition = np.argmax(neighbor_values)\n",
    "\n",
    "            if pi[state] != avail_actions[greedy_transition]:\n",
    "                policy_stable = False\n",
    "                pi[state] = avail_actions[greedy_transition]\n",
    "\n",
    "                \n",
    "        if policy_stable:\n",
    "            break\n",
    "        \n",
    "    return (V, pi)        # return both the final value function and the final policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, test our function using the MDP defined by gw.* functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values= [0.0, -1.0, -1.9, -2.71, -1.0, -1.9, -2.71, -1.9, -1.9, -2.71, -1.9, -1.0, -2.71, -1.9, -1.0]\n",
      "Policy= ['up', 'left', 'left', 'down', 'up', 'up', 'up', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\n"
     ]
    }
   ],
   "source": [
    "n_states = gw.get_state_count()\n",
    "\n",
    "# test our function\n",
    "values, policy = policy_iteration(state_count=n_states, gamma=.9, theta=.001, get_available_actions=gw.get_available_actions, \\\n",
    "    get_transitions=gw.get_transitions)\n",
    "\n",
    "print(\"Values=\", values)\n",
    "print(\"Policy=\", policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output from running above cell:**\n",
    "\n",
    "`\n",
    "Values= [0.0, -1.0, -1.9, -2.71, -1.0, -1.9, -2.71, -1.9, -1.9, -2.71, -1.9, -1.0, -2.71, -1.9, -1.0]\n",
    "Policy= ['up', 'left', 'left', 'down', 'up', 'up', 'up', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -1.  , -1.9 , -2.71],\n",
       "       [-1.  , -1.9 , -2.71, -1.9 ],\n",
       "       [-1.9 , -2.71, -1.9 , -1.  ],\n",
       "       [-2.71, -1.9 , -1.  ,  0.  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.append(values, 0)\n",
    "np.reshape(a, (4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, test our function using the test_dp helper.  The helper also uses the gw MDP, but with a different gamma value.\n",
    "If our function passes all tests, a passcode will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['up', 'left', 'left', 'down'],\n",
       "       ['up', 'up', 'up', 'down'],\n",
       "       ['up', 'up', 'down', 'down'],\n",
       "       ['up', 'right', 'right', 'up']], \n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.append(policy, policy[0])\n",
    "np.reshape(a, (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Policy Iteration\n",
      "passed test: return value is tuple\n",
      "passed test: length of tuple = 2\n",
      "passed test: v is list of length=15\n",
      "passed test: values of v elements\n",
      "passed test: pi is list of length=15\n",
      "passed test: values of pi elements\n",
      "PASSED: Policy Iteration passcode = 9970-010\n"
     ]
    }
   ],
   "source": [
    "# test our function using the test_db helper\n",
    "test_dp.policy_iteration_test( policy_iteration ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
