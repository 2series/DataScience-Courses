{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Text Classification with LSTM\n",
    "\n",
    "This lab corresponds to Module 6 of the \"Deep Learning Explained\" course.\n",
    "\n",
    "This lab shows how to implement a recurrent network to process text,\n",
    "for the [Air Travel Information Services](https://catalog.ldc.upenn.edu/LDC95S26) \n",
    "(ATIS) task of slot tagging (tag individual words to their respective classes, \n",
    "where the classes are provided as labels in the training data set).  \n",
    "\n",
    "Our model will start with a straight-forward (linear) embedding of the words followed by a recurrent LSTM.\n",
    "This will then be extended to include neighboring words and run bidirectionally.\n",
    "Lastly, we will turn this system into an intent classifier.  \n",
    "\n",
    "The techniques you will practice are:\n",
    "* model building by composing layer blocks, a convenient way to compose \n",
    "  networks/models without requiring the need to write formulas,\n",
    "* creating your own layer block\n",
    "* variables with different sequence lengths in the same network\n",
    "* training the network\n",
    "\n",
    "We assume that you are familiar with basics of deep learning, and these specific concepts:\n",
    "* recurrent networks ([Wikipedia page](https://en.wikipedia.org/wiki/Recurrent_neural_network))\n",
    "* text embedding ([Wikipedia page](https://en.wikipedia.org/wiki/Word_embedding))\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "We assume that you have already [installed CNTK](https://github.com/Microsoft/CNTK/wiki/Setup-CNTK-on-your-machine).\n",
    "This tutorial requires CNTK V2. We strongly recommend to run this tutorial on a machine with\n",
    "a capable CUDA-compatible GPU. Deep learning without GPUs is not fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the data\n",
    "\n",
    "In this tutorial we are going to use a (lightly preprocessed) version of the ATIS dataset. You can download the data automatically by running the cells below or by executing the manual instructions.\n",
    "\n",
    "#### Fallback manual instructions\n",
    "Download the ATIS [training](https://github.com/Microsoft/CNTK/blob/v2.0/Tutorials/SLUHandsOn/atis.train.ctf) \n",
    "and [test](https://github.com/Microsoft/CNTK/blob/v2.0/Tutorials/SLUHandsOn/atis.test.ctf) \n",
    "files and put them at the same folder as this notebook. If you want to see how the model is \n",
    "predicting on new sentences you will also need the vocabulary files for \n",
    "[queries](https://github.com/Microsoft/CNTK/blob/v2.0/Examples/LanguageUnderstanding/ATIS/BrainScript/query.wl) and\n",
    "[slots](https://github.com/Microsoft/CNTK/blob/v2.0/Examples/LanguageUnderstanding/ATIS/BrainScript/slots.wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing locally cached: slots.wl\n",
      "Reusing locally cached: atis.test.ctf\n",
      "Reusing locally cached: query.wl\n",
      "Reusing locally cached: atis.train.ctf\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility function to download a file \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "locations = ['Tutorials/SLUHandsOn', 'Examples/LanguageUnderstanding/ATIS/BrainScript']\n",
    "\n",
    "data = {\n",
    "  'train': { 'file': 'atis.train.ctf', 'location': 0 },\n",
    "  'test': { 'file': 'atis.test.ctf', 'location': 0 },\n",
    "  'query': { 'file': 'query.wl', 'location': 1 },\n",
    "  'slots': { 'file': 'slots.wl', 'location': 1 }\n",
    "}\n",
    "\n",
    "for item in data.values():\n",
    "    location = locations[item['location']]\n",
    "    path = os.path.join('..', location, item['file'])\n",
    "    if os.path.exists(path):\n",
    "        print(\"Reusing locally cached:\", item['file'])\n",
    "        # Update path\n",
    "        item['file'] = path\n",
    "    elif os.path.exists(item['file']):\n",
    "        print(\"Reusing locally cached:\", item['file'])\n",
    "    else:\n",
    "        print(\"Starting download:\", item['file'])\n",
    "        url = \"https://github.com/Microsoft/CNTK/blob/v2.0/%s/%s?raw=true\"%(location, item['file'])\n",
    "        download(url, item['file'])\n",
    "        print(\"Download completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing CNTK and other useful libraries\n",
    "\n",
    "CNTK's Python module contains several submodules like `io`, `learner`, and `layers`. We also use NumPy in some cases since the results returned by CNTK work like NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block below, we check if we are running this notebook in the CNTK internal test machines by looking for environment variables defined there. We then select the right target device (GPU vs CPU) to test this notebook. In other cases, we use CNTK's default policy to use the best available device (GPU, if available, else CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test for CNTK version\n",
    "if not C.__version__ == \"2.0\":\n",
    "    raise Exception(\"this notebook was designed to work with 2.0. Current Version: \" + C.__version__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and Model Structure\n",
    "\n",
    "The task we want to approach in this tutorial is slot tagging.\n",
    "We use the [ATIS corpus](https://catalog.ldc.upenn.edu/LDC95S26).\n",
    "ATIS contains human-computer queries from the domain of Air Travel Information Services,\n",
    "and our task will be to annotate (tag) each word of a query with the specific item of information (slot) it belongs to, if any.\n",
    "\n",
    "The data in your working folder has already been converted into a CTF (CNTK Text Format) file.\n",
    "Let us look at an example from the test-set file `atis.test.ctf`:\n",
    "\n",
    "    19  |S0 178:1 |# BOS      |S1 14:1 |# flight  |S2 128:1 |# O\n",
    "    19  |S0 770:1 |# show                         |S2 128:1 |# O\n",
    "    19  |S0 429:1 |# flights                      |S2 128:1 |# O\n",
    "    19  |S0 444:1 |# from                         |S2 128:1 |# O\n",
    "    19  |S0 272:1 |# burbank                      |S2 48:1  |# B-fromloc.city_name\n",
    "    19  |S0 851:1 |# to                           |S2 128:1 |# O\n",
    "    19  |S0 789:1 |# st.                          |S2 78:1  |# B-toloc.city_name\n",
    "    19  |S0 564:1 |# louis                        |S2 125:1 |# I-toloc.city_name\n",
    "    19  |S0 654:1 |# on                           |S2 128:1 |# O\n",
    "    19  |S0 601:1 |# monday                       |S2 26:1  |# B-depart_date.day_name\n",
    "    19  |S0 179:1 |# EOS                          |S2 128:1 |# O\n",
    "\n",
    "This file has 5-7 columns per line (each separated by the \"|\" character):\n",
    "\n",
    "* a sequence id (19). There are 11 entries with this sequence id. This means that sequence 19 consists\n",
    "of 11 tokens;\n",
    "* column `S0`, which contains numeric word indices; the input data is encoded in one-hot vectors.  There are 943 words in the vocabulary, so each word is a 943 element vector of all 0 with a 1 at a vector index chosen to represent that word.  For example the word \"from\" is represented with a 1 at index 444 and zero everywhere else in the vector. The word \"monday\" is represented with a 1 at index 601 and zero everywhere else in the vector.\n",
    "* a comment column denoted by `#`, to allow a human reader to know what the numeric word index stands for;\n",
    "Comment columns are ignored by the system. `BOS` and `EOS` are special words\n",
    "to denote beginning and end of sentence, respectively;\n",
    "* column `S1` is an intent label, which we will only use in the last part of the tutorial;\n",
    "* another comment column that shows the human-readable label of the numeric intent index;\n",
    "* column `S2` is the slot label, represented as a numeric index; and\n",
    "* another comment column that shows the human-readable label of the numeric label index.\n",
    "\n",
    "The task of the neural network is to look at the query (column `S0`) and predict the\n",
    "slot label (column `S2`).\n",
    "As you can see, each word in the input gets assigned either an empty label `O`\n",
    "or a slot label that begins with `B-` for the first word, and with `I-` for any\n",
    "additional consecutive word that belongs to the same slot.\n",
    "\n",
    "The model we will use is a recurrent model consisting of an embedding layer,\n",
    "a recurrent LSTM cell, and a dense layer to compute the posterior probabilities:\n",
    "\n",
    "\n",
    "    slot label   \"O\"        \"O\"        \"O\"        \"O\"  \"B-fromloc.city_name\"\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "              | Dense |  | Dense |  | Dense |  | Dense |  | Dense |  ...\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +------+   +------+   +------+   +------+   +------+   \n",
    "         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->...\n",
    "              +------+   +------+   +------+   +------+   +------+   \n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "              | Embed |  | Embed |  | Embed |  | Embed |  | Embed |  ...\n",
    "              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                  ^          ^          ^          ^          ^\n",
    "                  |          |          |          |          |\n",
    "    w      ------>+--------->+--------->+--------->+--------->+------... \n",
    "                 BOS      \"show\"    \"flights\"    \"from\"   \"burbank\"\n",
    "\n",
    "Descriptions of the above Layer functions can be found at: [the CNTK Layers Reference Documentation](http://cntk.ai/pythondocs/layerref.html).\n",
    "\n",
    "Below, we build the CNTK model for this network. Please have a quick look and match it with the description above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting seed\n",
    "np.random.seed(0)\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = 943 ; num_labels = 129 ; num_intents = 26    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "\n",
    "#emb_dim    = 50\n",
    "emb_dim    = 150\n",
    "#emb_dim    = 300\n",
    "\n",
    "#hidden_dim = 100\n",
    "hidden_dim = 300\n",
    "#hidden_dim = 500\n",
    "\n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.sequence.input_variable(num_labels)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a model and inspect it. \n",
    "\n",
    "Once a model is constructed, its attributes are fully accessible from Python. The first layer named `embed` is an Embedding layer. Here we use the CNTK default, which is linear embedding. It is a simple matrix with dimension (input word encoding x output projected dimension). You can access its parameter `E` (where the embeddings are stored) like any other attribute of a Python object. Its shape contains a `-1` which indicates that this parameter (with input dimension) is not fully specified yet, while the output dimension is set to `emb_dim` ( = 150 in this tutorial). \n",
    "\n",
    "Additionally we also inspect the value of the bias vector in the `Dense` layer named `classify`. The `Dense` layer is a fundamental compositional unit of a Multi-Layer Perceptron (as introduced in Lab 3). Each `Dense` layer has both `weight` and `bias` parameters.  Bias terms are by default initialized to 0 (but there is a way to change that if you need). As you create the model, one can name the layer component and then access the parameters as shown here. \n",
    "\n",
    "**Suggested Exploration**: What should be the expected dimension of the `weight` matrix from the layer named `classify`.  Try printing the weight matrix of the `classify` layer.  Does it match with your expected size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 150)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "(-1, 129)\n"
     ]
    }
   ],
   "source": [
    "# peek\n",
    "z = create_model()\n",
    "print(z.embed.E.shape)\n",
    "print(z.classify.b.value)\n",
    "print(z.classify.W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input text words will be encoded as one-hot vectors of length 943 and the output dimension of our model `emb_dim` is set to 150. In the code below we pass the input variable `x` to our model `z`. This binds the model with input data of known shape. In this case, the input shape will be the size of the input vocabulary. With this modification, the parameter returned by the embed layer is completely specified (943, 150). \n",
    "\n",
    "**Note**: As an alternative to our approach here, you can initialize the Embedding matrix with pre-computed vectors using [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) or [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 150)\n",
      "(300, 129)\n"
     ]
    }
   ],
   "source": [
    "# Pass an input and check the dimension\n",
    "z = create_model()\n",
    "print(z(x).embed.E.shape)\n",
    "print(z(x).classify.W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[comment]: <> (For testing ...)\n",
    "\n",
    "### A Brief Look at Data and Data Reading\n",
    "\n",
    "For reading text, this tutorial uses the `CNTKTextFormatReader`. It expects the input data to be\n",
    "in the CTF format, as described [here](https://github.com/Microsoft/CNTK/wiki/CNTKTextFormat-Reader).\n",
    "\n",
    "But how do you generate this format?  For this tutorial, we created the CTF file for you, but it might be helpful to explain how this was accomplished.  The data was created in two steps:\n",
    "* convert the raw data into a plain text file that contains of TAB-separated columns of space-separated text. For example:\n",
    "\n",
    "  ```\n",
    "  BOS show flights from burbank to st. louis on monday EOS (TAB) flight (TAB) O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O B-depart_date.day_name O\n",
    "  ```\n",
    "\n",
    "  This is meant to be compatible with the output of the `paste` command.\n",
    "  \n",
    "  \n",
    "* convert it to CNTK Text Format (CTF) with the following command:\n",
    "\n",
    "  ```\n",
    "  python [CNTK root]/Scripts/txt2ctf.py --map query.wl intent.wl slots.wl --annotated True --input atis.test.txt --output atis.test.ctf\n",
    "  ```\n",
    "  where the three `.wl` files give the vocabulary as plain text files, one word per line.\n",
    "\n",
    "In these CTF files, our columns are labeled `S0`, `S1`, and `S2`.\n",
    "These are connected to the actual network inputs by the corresponding lines in the reader definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent_unused = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = C.io.StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['slot_labels', 'intent_unused', 'query'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek\n",
    "reader = create_reader(data['train']['file'], is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "We also must define the training criterion (loss function), and also an error metric to track the progress of our model's performance. In most tutorials, we know the input dimensions and the corresponding labels. We directly create the loss and the error functions. In this tutorial we will do the same. However, we take a brief detour and learn about placeholders. This concept would be useful for Task 3. \n",
    "\n",
    "**Learning note**: Introduction to `placeholder`: Remember that the code we have been writing is not actually executing any heavy computation it is just specifying the function we want to compute on data during training/testing. And in the same way that it is convenient to have names for arguments when you write a regular function in a programming language, it is convenient to have placeholders that refer to arguments (or local computations that need to be reused). Eventually, some other code will replace these placeholders with other known quantities in the same way that in a programming language the function will be called with concrete values bound to its arguments. \n",
    "\n",
    "Specifically, the input variables you have created above `x = C.sequence.input_variable(vocab_size)` holds data pre-defined by `vocab_size`. In the case where such instantiations are challenging or not possible, using `placeholder` is a logical choice. Having the `placeholder` only allows you to defer the specification of the argument at a later time when you may have the data.\n",
    "\n",
    "Here is an example below that illustrates the use of `placeholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(Combine): Input('Input2465', [#, *], [129]), Placeholder('labels', [???], [???]) -> Output('Block2435_Output_0', [#, *], [1]), Output('Block2455_Output_0', [#, *], [])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_criterion_function(model):\n",
    "    labels = C.placeholder(name='labels')\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return C.combine ([ce, errs]) # (features, labels) -> (loss, metric)\n",
    "\n",
    "criterion = create_criterion_function(create_model())\n",
    "criterion.replace_placeholders({criterion.placeholders[0]: C.sequence.input_variable(num_labels)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the cell above works well when one has input parameters defined at network creation, it compromises readability. Hence we prefer creating functions as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We are using the Progress Printer to display the training loss and classification error throughout training epochs.\n",
    "\n",
    "The training should take less than 2 minutes on a Titan-X or a Surface Book. Once the training completed, you will see an output like this\n",
    "```\n",
    "Finished Epoch [10]: [Training] loss = 0.033263 * 18039, metric = 0.9% * 18039\n",
    "```\n",
    "which is the loss (cross entropy) and the metric (classification error) averaged over the final epoch.\n",
    "\n",
    "On a CPU-only machine, it can be 4 or more times slower. You can try setting\n",
    "```python\n",
    "emb_dim    = 50 \n",
    "hidden_dim = 100\n",
    "```\n",
    "to reduce the time it takes to run on a CPU, but the model will not fit as well as when the \n",
    "hidden and embedding dimension are larger. \n",
    "\n",
    "### Testing the model\n",
    "\n",
    "We also use the Progress Printer to display the accuracy on a test set by computing the error over multiple minibatches of test data. For evaluating on a small sample read from a file, you can set a minibatch size reflecting the sample size and run the test_minibatch on that instance of data. To see how to evaluate a single sequence, we provide an instance later in the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, max_epochs=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_minibatch, C.UnitType.minibatch, epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentum_as_time_constant,\n",
    "                     gradient_clipping_threshold_per_sample=15, \n",
    "                     gradient_clipping_with_truncation=True)\n",
    "\n",
    "    #learner = C.sgd(parameters=model.parameters, lr=lr_schedule)\n",
    "    \n",
    "    #learner = C.fsadagrad(parameters = model.parameters, lr = lr_schedule, momentum=momentum_as_time_constant)\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs) \n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                x: train_reader.streams.query,\n",
    "                y: train_reader.streams.slot_labels\n",
    "            })\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()\n",
    "    \n",
    "    while True:\n",
    "        minibatch_size = 500\n",
    "        data = test_reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            x: test_reader.streams.query,\n",
    "            y: test_reader.streams.slot_labels\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "        trainer.test_minibatch(data)\n",
    "    \n",
    "    trainer.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_train_test():\n",
    "    global z\n",
    "    z = create_model()\n",
    "    train_reader = create_reader(data['train']['file'], is_training=True)\n",
    "    test_reader = create_reader(data['test']['file'], is_training=False)\n",
    "    train_test(train_reader, test_reader, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 721479 parameters in 6 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.823758 * 18010, metric = 16.06% * 18010 10.471s (1720.0 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.237414 * 18051, metric = 5.37% * 18051 11.256s (1603.7 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.161291 * 17941, metric = 3.79% * 17941 7.936s (2260.7 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.112823 * 18059, metric = 2.65% * 18059 9.898s (1824.5 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.076638 * 17957, metric = 1.90% * 17957 9.293s (1932.3 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.065043 * 18021, metric = 1.62% * 18021 8.272s (2178.6 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.057932 * 17980, metric = 1.50% * 17980 7.948s (2262.2 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.054805 * 18025, metric = 1.43% * 18025 8.402s (2145.3 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.036560 * 17956, metric = 1.06% * 17956 10.036s (1789.2 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.036755 * 18039, metric = 0.97% * 18039 10.523s (1714.2 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-23]: metric = 2.44% * 10984;\n"
     ]
    }
   ],
   "source": [
    "do_train_test()\n",
    "\n",
    "#Base case, embedding dimension  = 150  and hidden dimension = 300\n",
    "#Finished Evaluation [1]: Minibatch[1-23]: metric = 2.44% * 10984;\n",
    "\n",
    "# Embedding dimension  = 50  and hidden dimension = 300\n",
    "#Finished Evaluation [1]: Minibatch[1-23]: metric = 2.62% * 10984;\n",
    "\n",
    "# Embedding dimension  = 300  and hidden dimension = 300\n",
    "#Finished Evaluation [1]: Minibatch[1-23]: metric = 2.28% * 10984;\n",
    "\n",
    "# Embedding dimension  = 150  and hidden dimension = 100\n",
    "#Finished Evaluation [1]: Minibatch[1-23]: metric = 2.45% * 10984;\n",
    "\n",
    "# Embedding dimension  = 150  and hidden dimension = 500\n",
    "#Finished Evaluation [1]: Minibatch[1-23]: metric = 2.23% * 10984;\n",
    "\n",
    "# Learner - sgd, embedding dimension = 150, hidden dimension = 300\n",
    "# Finished Evaluation [1]: Minibatch[1-23]: metric = 11.83% * 10984;\n",
    "\n",
    "# Learner - fsadagrad, embedding dimension = 150, hidden dimension = 300\n",
    "#Finished Evaluation [1]: Minibatch[1-23]: metric = 3.28% * 10984;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how learning proceeds over epochs (passes through the data).\n",
    "For example, after four epochs, the loss, which is the cross-entropy criterion, has reached 0.11 as measured on the ~18000 samples of this epoch,\n",
    "and that the error rate is 2.6% on those same 18000 training samples.\n",
    "\n",
    "The epoch size is the number of samples--counted as *word tokens*, not sentences--to\n",
    "process between model checkpoints.\n",
    "\n",
    "Once the training has completed (a little less than 2 minutes on a Titan-X or a Surface Book),\n",
    "you will see an output like this\n",
    "```\n",
    "Finished Epoch [10]: [Training] loss = 0.033263 * 18039, metric = 0.9% * 18039\n",
    "```\n",
    "which is the loss (cross entropy) and the metric (classification error) averaged over the final epoch.\n",
    "\n",
    "On a CPU-only machine, it can be 4 or more times slower. You can try setting\n",
    "```python\n",
    "emb_dim    = 50 \n",
    "hidden_dim = 100\n",
    "```\n",
    "to reduce the time it takes to run on a CPU, but the model will not fit as well as when the \n",
    "hidden and embedding dimension are larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.27747105e-02,  -6.29460216e-02,  -2.14031097e-02,\n",
       "         1.70911569e-02,  -1.09594781e-02,   1.59418036e-04,\n",
       "        -2.28475453e-03,  -3.94062735e-02,   2.18422916e-02,\n",
       "        -2.42655426e-02,   9.53336880e-02,   4.30206880e-02,\n",
       "        -6.51988089e-02,   2.98207924e-02,  -7.08244517e-02,\n",
       "        -8.50695297e-02,  -8.21969584e-02,  -2.61888131e-02,\n",
       "         1.96841289e-03,  -8.91644880e-02,  -2.16846541e-03,\n",
       "        -3.19706574e-02,   4.83303778e-02,   2.62612440e-02,\n",
       "         6.58376375e-03,  -2.44466942e-02,   3.42787942e-03,\n",
       "        -7.89799690e-02,   1.10458070e-02,  -4.72935475e-02,\n",
       "        -2.81648114e-02,   5.40100671e-02,  -5.91306500e-02,\n",
       "        -1.01642376e-02,   4.11498696e-02,   1.15170386e-02,\n",
       "        -5.13490429e-03,  -6.43310845e-02,   9.38927196e-03,\n",
       "        -7.80601054e-02,  -8.74073133e-02,  -8.94277692e-02,\n",
       "        -1.17708631e-02,  -8.22204798e-02,  -5.88645339e-02,\n",
       "        -4.81638871e-02,   3.87195535e-02,   3.90707664e-02,\n",
       "         3.14336047e-02,  -7.21811205e-02,  -5.47366813e-02,\n",
       "        -4.74207997e-02,   5.17152697e-02,  -5.59183732e-02,\n",
       "         5.70636056e-02,   3.41333635e-02,  -7.82287866e-02,\n",
       "        -1.57154482e-02,  -1.19047957e-02,  -5.59309730e-03,\n",
       "         5.53550944e-02,   2.37968261e-03,   4.16482538e-02,\n",
       "        -1.09653343e-02,   1.30150402e-02,   3.21998410e-02,\n",
       "        -1.24094270e-01,  -2.35084575e-02,   1.43548464e-02,\n",
       "        -8.69137198e-02,  -1.00355623e-02,  -2.74364259e-02,\n",
       "         7.94172194e-03,  -9.81708895e-03,  -3.84137034e-02,\n",
       "         3.73459309e-02,   8.79630521e-02,   9.01166424e-02,\n",
       "         5.58554530e-02,  -5.49879298e-03,  -6.99062794e-02,\n",
       "        -3.29038985e-02,  -5.38548380e-02,  -1.96140081e-01,\n",
       "        -7.06117451e-02,  -3.63565385e-02,  -1.01162903e-02,\n",
       "         5.99694587e-02,  -1.20276343e-02,  -1.07726082e-01,\n",
       "        -1.98301226e-02,  -9.57928747e-02,  -1.22565024e-01,\n",
       "        -1.68472841e-01,  -1.22539075e-02,  -8.17641467e-02,\n",
       "         5.57617471e-02,  -3.22690383e-02,  -5.02388552e-02,\n",
       "        -8.21062848e-02,  -2.06451919e-02,  -4.14834768e-02,\n",
       "        -6.30375519e-02,  -1.02921396e-01,   2.29076395e-04,\n",
       "         3.86827961e-02,  -7.69258440e-02,  -6.04634658e-02,\n",
       "        -1.03579663e-01,  -3.47700715e-02,  -1.07512847e-01,\n",
       "        -3.44051700e-03,   1.82225592e-02,   3.04303523e-02,\n",
       "         1.92961767e-02,  -7.74645358e-02,   2.59252060e-02,\n",
       "         2.55911835e-02,   1.65601317e-02,  -1.70889586e-01,\n",
       "        -8.69644433e-02,  -3.11060008e-02,  -4.17364053e-02,\n",
       "         3.73811014e-02,  -1.45075284e-02,  -3.41120437e-02,\n",
       "         8.50370433e-03,  -9.66504142e-02,   5.87689765e-02], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.classify.b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code illustrates how to evaluate a single sequence. Additionally we show how one can pass in the information using NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178, 429, 444, 619, 937, 851, 752, 179]\n",
      "One Hot shape: (8, 943)\n",
      "Prediction Shape: (8, 129)\n",
      "[128 128 128  48 110 128  78 128]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOS', 'O'),\n",
       " ('flights', 'O'),\n",
       " ('from', 'O'),\n",
       " ('new', 'B-fromloc.city_name'),\n",
       " ('york', 'I-fromloc.city_name'),\n",
       " ('to', 'O'),\n",
       " ('seattle', 'B-toloc.city_name'),\n",
       " ('EOS', 'O')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "slots_wl = [line.rstrip('\\n') for line in open(data['slots']['file'])]\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "\n",
    "# let's run a sequence through\n",
    "seq = 'BOS flights from new york to seattle EOS'\n",
    "w = [query_dict[w] for w in seq.split()] # convert to word indices\n",
    "print(w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "print('One Hot shape: ' + str(onehot.shape))\n",
    "#x = C.sequence.input_variable(vocab_size)\n",
    "pred = z(x).eval({x:[onehot]})[0]\n",
    "print('Prediction Shape: ' + str(pred.shape))\n",
    "best = np.argmax(pred,axis=1)\n",
    "print(best)\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code will throw and error as paris is not in the vocab and thus the lookuo from query_dict will fail\n",
    "#seq = 'BOS flights from new york to paris EOS'\n",
    "#w = [query_dict[w] for w in seq.split()] # convert to word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOS', 'O'),\n",
       " ('i', 'O'),\n",
       " ('want', 'O'),\n",
       " ('round', 'B-round_trip'),\n",
       " ('trip', 'I-round_trip'),\n",
       " ('flights', 'O'),\n",
       " ('from', 'O'),\n",
       " ('new', 'B-fromloc.city_name'),\n",
       " ('york', 'I-fromloc.city_name'),\n",
       " ('to', 'O'),\n",
       " ('toronto', 'B-toloc.city_name'),\n",
       " ('EOS', 'O')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = 'BOS i want round trip flights from new york to toronto EOS'\n",
    "w = [query_dict[word] for word in seq.split()]\n",
    "onehot = np.zeros((len(w), len(query_dict)), dtype = np.float32)\n",
    "for i in range(len(w)):\n",
    "    onehot[i, w[i]] = 1\n",
    "\n",
    "pred = z(x).eval({x: onehot})[0]\n",
    "list(zip(seq.split(), [slots_wl[idx] for idx in pred.argmax(axis = 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Word About [`Sequential()`](https://www.cntk.ai/pythondocs/layerref.html#sequential)\n",
    "\n",
    "Before jumping to the tasks, let's have a look again at the model we just ran.\n",
    "The model is described in what we call *function-composition style*.\n",
    "```python\n",
    "        Sequential([\n",
    "            Embedding(emb_dim),\n",
    "            Recurrence(LSTM(hidden_dim), go_backwards=False),\n",
    "            Dense(num_labels)\n",
    "        ])\n",
    "```\n",
    "You may be familiar with the \"sequential\" notation from other neural-network toolkits.\n",
    "If not, [`Sequential()`](https://www.cntk.ai/pythondocs/layerref.html#sequential) is a powerful operation that,\n",
    "in a nutshell, allows to compactly express a very common situation in neural networks\n",
    "where an input is processed by propagating it through a progression of layers.\n",
    "`Sequential()` takes an list of functions as its argument,\n",
    "and returns a *new* function that invokes these functions in order,\n",
    "each time passing the output of one to the next.\n",
    "For example,\n",
    "```python\n",
    "\tFGH = Sequential ([F,G,H])\n",
    "    y = FGH (x)\n",
    "```\n",
    "means the same as\n",
    "```\n",
    "    y = H(G(F(x))) \n",
    "```\n",
    "This is known as [\"function composition\"](https://en.wikipedia.org/wiki/Function_composition),\n",
    "and is especially convenient for expressing neural networks, which often have this form:\n",
    "\n",
    "         +-------+   +-------+   +-------+\n",
    "    x -->|   F   |-->|   G   |-->|   H   |--> y\n",
    "         +-------+   +-------+   +-------+\n",
    "\n",
    "Coming back to our model at hand, the `Sequential` expression simply\n",
    "says that our model has this form:\n",
    "\n",
    "         +-----------+   +----------------+   +------------+\n",
    "    x -->| Embedding |-->| Recurrent LSTM |-->| DenseLayer |--> y\n",
    "         +-----------+   +----------------+   +------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Add a Lookahead \n",
    "\n",
    "Our recurrent model suffers from a structural deficit:\n",
    "Since the recurrence runs from left to right, the decision for a slot label\n",
    "has no information about upcoming words. The model is a bit lopsided.\n",
    "Your task will be to modify the model such that\n",
    "the input to the recurrence consists not only of the current word, but also of the next one\n",
    "(lookahead).\n",
    "\n",
    "Your solution should be in function-composition style.\n",
    "Hence, you will need to write a Python function that does the following:\n",
    "\n",
    "* takes no input arguments\n",
    "* creates a placeholder (sequence) variable\n",
    "* computes the \"next value\" in this sequence using the `sequence.future_value()` operation and\n",
    "* concatenates the current and the next value into a vector of twice the embedding dimension using `splice()`\n",
    "\n",
    "and then insert this function into `Sequential()`'s list right after the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 901479 parameters in 6 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.733090 * 18010, metric = 14.28% * 18010 10.849s (1660.1 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.194045 * 18051, metric = 4.37% * 18051 8.025s (2249.3 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.129212 * 17941, metric = 2.83% * 17941 7.743s (2317.1 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.085530 * 18059, metric = 2.00% * 18059 10.112s (1785.9 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.049946 * 17957, metric = 1.16% * 17957 10.384s (1729.3 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.046134 * 18021, metric = 0.99% * 18021 9.455s (1906.0 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.045283 * 17980, metric = 1.13% * 17980 8.812s (2040.4 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.034170 * 18025, metric = 0.88% * 18025 8.640s (2086.2 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.018857 * 17956, metric = 0.47% * 17956 9.027s (1989.1 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.019557 * 18039, metric = 0.52% * 18039 8.239s (2189.5 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-23]: metric = 1.99% * 10984;\n"
     ]
    }
   ],
   "source": [
    "# Your task: Add lookahead\n",
    "#TODO: Understand Sequential in more depth, the internals\n",
    "def oneWordLookAhead():\n",
    "    x = C.placeholder()\n",
    "    return C.splice(x, C.sequence.future_value(x))\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            oneWordLookAhead(),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels)\n",
    "        ])\n",
    "    \n",
    "# Enable these when done:\n",
    "z = create_model()\n",
    "do_train_test()\n",
    "\n",
    "#After adding look ahead\n",
    "#Finished Evaluation [1]: Minibatch[1-23]: metric = 1.99% * 10984;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Bidirectional Recurrent Model\n",
    "\n",
    "Aha, knowledge of future words help. So instead of a one-word lookahead,\n",
    "why not look ahead until all the way to the end of the sentence, through a backward recurrence?\n",
    "Let us create a bidirectional model!\n",
    "\n",
    "Your task is to implement a new layer that\n",
    "performs both a forward and a backward recursion over the data, and\n",
    "concatenates the output vectors.\n",
    "\n",
    "Note, however, that this differs from the previous task in that\n",
    "the bidirectional layer contains learnable model parameters.\n",
    "In function-composition style,\n",
    "the pattern to implement a layer with model parameters is to write a *factory function*\n",
    "that creates a *function object*.\n",
    "\n",
    "A function object, also known as [*functor*](https://en.wikipedia.org/wiki/Function_object), is an object that is both a function and an object.\n",
    "Which means nothing else that it contains data yet still can be invoked as if it was a function.\n",
    "\n",
    "For example, `Dense(outDim)` is a factory function that returns a function object that contains\n",
    "a weight matrix `W`, a bias `b`, and another function to compute \n",
    "`input @ W + b.` (This is using \n",
    "[Python 3.5 notation for matrix multiplication](https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465).\n",
    "In Numpy syntax it is `input.dot(W) + b`).\n",
    "E.g. saying `Dense(1024)` will create this function object, which can then be used\n",
    "like any other function, also immediately: `Dense(1024)(x)`. \n",
    "\n",
    "Let's look at an example for further clarity: Let us implement a new layer that combines\n",
    "a linear layer with a subsequent batch normalization. \n",
    "To allow function composition, the layer needs to be realized as a factory function,\n",
    "which could look like this:\n",
    "\n",
    "```python\n",
    "def DenseLayerWithBN(dim):\n",
    "    F = Dense(dim)\n",
    "    G = BatchNormalization()\n",
    "    x = placeholder()\n",
    "    apply_x = G(F(x))\n",
    "    return apply_x\n",
    "```\n",
    "\n",
    "Invoking this factory function will create `F`, `G`, `x`, and `apply_x`. In this example, `F` and `G` are function objects themselves, and `apply_x` is the function to be applied to the data.\n",
    "Thus, e.g. calling `DenseLayerWithBN(1024)` will\n",
    "create an object containing a linear-layer function object called `F`, a batch-normalization function object `G`,\n",
    "and `apply_x` which is the function that implements the actual operation of this layer\n",
    "using `F` and `G`. It will then return `apply_x`. To the outside, `apply_x` looks and behaves\n",
    "like a function. Under the hood, however, `apply_x` retains access to its specific instances of `F` and `G`.\n",
    "\n",
    "Now back to our task at hand. You will now need to create a factory function,\n",
    "very much like the example above.\n",
    "You shall create a factory function\n",
    "that creates two recurrent layer instances (one forward, one backward), and then defines an `apply_x` function\n",
    "which applies both layer instances to the same `x` and concatenate the two results.\n",
    "\n",
    "Alright, give it a try! To know how to realize a backward recursion in CNTK,\n",
    "please take a hint from how the forward recursion is done.\n",
    "Please also do the following:\n",
    "* remove the one-word lookahead you added in the previous task, which we aim to replace; and\n",
    "* make sure each LSTM is using `hidden_dim//2` outputs to keep the total number of model parameters limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 541479 parameters in 9 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.731444 * 18010, metric = 13.83% * 18010 6.723s (2678.9 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.175613 * 18051, metric = 3.90% * 18051 6.698s (2695.0 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.103988 * 17941, metric = 2.33% * 17941 6.150s (2917.2 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.070100 * 18059, metric = 1.59% * 18059 6.231s (2898.3 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.042498 * 17957, metric = 1.10% * 17957 6.473s (2774.1 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.037199 * 18021, metric = 0.88% * 18021 6.478s (2781.9 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.037888 * 17980, metric = 0.93% * 17980 8.105s (2218.4 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.027862 * 18025, metric = 0.70% * 18025 7.426s (2427.3 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.014863 * 17956, metric = 0.39% * 17956 7.128s (2519.1 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.015637 * 18039, metric = 0.47% * 18039 6.329s (2850.2 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-23]: metric = 1.93% * 10984;\n"
     ]
    }
   ],
   "source": [
    "#TODO: Understand these Sequentials and internals in more depth\n",
    "def BiDirectional(fwd, rev):\n",
    "    x = C.placeholder()\n",
    "    f = C.layers.Recurrence(fwd, go_backwards= False)\n",
    "    r = C.layers.Recurrence(rev, go_backwards= True)\n",
    "    return C.splice(f(x), r(x))\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):  \n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            BiDirectional(C.layers.LSTM(hidden_dim // 2), C.layers.LSTM(hidden_dim // 2)),\n",
    "            C.layers.Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "# Enable these when done:\n",
    "z = create_model()\n",
    "do_train_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works like a charm! This model achieves 0.32%, better than the lookahead model above.\n",
    "The bidirectional model has 40% less parameters than the lookahead one. However, if you go back and look closely\n",
    "you may find that the lookahead one trained about 30% faster.\n",
    "This is because the lookahead model has both less horizontal dependencies (one instead of two\n",
    "recurrences) and larger matrix products, and can thus achieve higher parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter('W', [], [300 x 129]),\n",
       " Parameter('b', [], [129]),\n",
       " Parameter('b', [], [600]),\n",
       " Parameter('W', [], [150 x 600]),\n",
       " Parameter('H', [], [150 x 600]),\n",
       " Parameter('E', [], [943 x 150]),\n",
       " Parameter('b', [], [600]),\n",
       " Parameter('W', [], [150 x 600]),\n",
       " Parameter('H', [], [150 x 600]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
